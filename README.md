# MobileVLA: Vision Language Action Model for Mobile Devices

[![Apache License](https://img.shields.io/badge/license-Apache-green.svg)](https://opensource.org/licenses/MIT) [![LICENSE](https://img.shields.io/badge/license-Anti%20996-blue.svg)](https://github.com/996icu/996.ICU/blob/master/LICENSE)



### Requirements

- [MobileVLM-v2](https://github.com/Meituan-AutoML/MobileVLM)



### Datasets and Benchmarks

- [CALVIN](http://calvin.cs.uni-freiburg.de/)



### Reference

[2023] [VIMA: General Robot Manipulation with Multimodal Prompts](https://arxiv.org/abs/2210.03094)

[2024] [Vision-Language Foundation Models as Effective Robot Imitators](https://arxiv.org/abs/2311.01378)

[2023] [MobileVLM : A Fast, Strong and Open Vision Language Assistant for Mobile Devices](https://arxiv.org/abs/2312.16886)

[2024] [MobileVLM V2: Faster and Stronger Baseline for Vision Language Model](https://arxiv.org/abs/2402.03766)

[2024] [MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases](https://arxiv.org/abs/2402.14905)

[2024] [Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models](https://arxiv.org/abs/2403.18814)