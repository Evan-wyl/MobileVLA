# MobileVLA: Vision Language Action Model for Mobile Devices

[![Apache License](https://img.shields.io/badge/license-Apache-green.svg)](https://opensource.org/licenses/MIT) [![LICENSE](https://img.shields.io/badge/license-Anti%20996-blue.svg)](https://github.com/996icu/996.ICU/blob/master/LICENSE)

MobileVLA is expected to be a smart assistant by deploying on mobile devices.



## Prerequisite:

### Step 0

Download the [CALVIN](https://github.com/mees/calvin) dataset



## Acknowledgment

#### CALVIN

Original: https://github.com/mees/calvin License: [MIT](https://github.com/mees/calvin/blob/main/LICENSE)

#### ByteDance RoboFlamingo

Original: https://github.com/RoboFlamingo/RoboFlamingo License: [MIT](https://github.com/RoboFlamingo/RoboFlamingo/blob/main/LICENSE)

#### Meituan MobileVLM

Original: https://github.com/Meituan-AutoML/MobileVLM License: [Apache-2.0](https://github.com/Meituan-AutoML/MobileVLM/blob/main/LICENSE)



### Reference

- [Vision-Language Foundation Models as Effective Robot Imitators](https://arxiv.org/abs/2311.01378)
- [MobileVLM : A Fast, Strong and Open Vision Language Assistant for Mobile Devices](https://arxiv.org/abs/2312.16886)
- [MobileVLM V2: Faster and Stronger Baseline for Vision Language Model](https://arxiv.org/abs/2402.03766)
- [MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases](https://arxiv.org/abs/2402.14905)
- [Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models](https://arxiv.org/abs/2403.18814)

- [VIMA: General Robot Manipulation with Multimodal Prompts](https://arxiv.org/abs/2210.03094)